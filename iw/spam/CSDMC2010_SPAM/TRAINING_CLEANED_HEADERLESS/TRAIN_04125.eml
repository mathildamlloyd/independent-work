>   tp> i'm reading this now as that you trained on about 220 spam and
>   tp> about 220 ham.  that's less than 10% of the sizes of the
>   tp> training sets i've been using.  please try an experiment: train
>   tp> on 550 of each, and test once against the other 550 of each.

[jeremy]
> this helps a lot!

possibly.  i checked in a change to classifier.py overnight (getting rid of
mincount) that gave a major improvement in the f-n rate too, independent of
tokenization.

> here are results with the stock tokenizer:

unsure what "stock tokenizer" means to you.  for example, it might mean
tokenizer.tokenize, or mboxtest.mytokenizer.tokenize.

> training on  &  /home/jeremy/mail/spam 8>
>  ... 644 hams & 557 spams
>       0.000  10.413
>       1.398   6.104
>       1.398   5.027
> training on  &  /home/jeremy/mail/spam 0>
>  ... 644 hams & 557 spams
>       0.000   8.259
>       1.242   2.873
>       1.242   5.745
> training on  &  /home/jeremy/mail/spam 3>
>  ... 644 hams & 557 spams
>       1.398   5.206
>       1.398   4.488
>       0.000   9.336
> training on  &  /home/jeremy/mail/spam 0>
>  ... 644 hams & 557 spams
>       1.553   5.206
>       1.553   5.027
>       0.000   9.874
> total false pos 139 5.39596273292
> total false neg 970 43.5368043088

note that those rates remain much higher than i got using just 220 of ham
and 220 of spam.  that remains a mystery.

> and results from the tokenizer that looks at all headers except date,
> received, and x-from_:

unsure what that means too.  for example, "looks at" might mean you enabled
anthony's count-them gimmick, and/or that you're tokenizing them yourself,
and/or ...?

> training on  &  /home/jeremy/mail/spam 8>
>  ... 644 hams & 557 spams
>       0.000   7.540
>       0.932   4.847
>       0.932   3.232
> training on  &  /home/jeremy/mail/spam 0>
>  ... 644 hams & 557 spams
>       0.000   7.181
>       0.621   2.873
>       0.621   4.847
> training on  &  /home/jeremy/mail/spam 3>
>  ... 644 hams & 557 spams
>       1.087   4.129
>       1.087   3.052
>       0.000   6.822
> training on  &  /home/jeremy/mail/spam 0>
>  ... 644 hams & 557 spams
>       0.776   3.411
>       0.776   3.411
>       0.000   6.463
> total false pos 97 3.76552795031
> total false neg 738 33.1238779174
>
> is it safe to conclude that avoiding any cleverness with headers is a
> good thing?

since i don't know what you did, exactly, i can't guess.  what you seemed to
show is that you did *something* clever with headers and that doing so
helped (the "after" numbers are better than the "before" numbers, right?).
assuming that what you did was override what's now
tokenizer.tokenizer.tokenize_headers with some other routine, and didn't
call the base tokenizer.tokenize_headers at all, then you're missing
carefully tested treatment of just a few header fields, but adding many
dozens of other header fields.  there's no question that adding more header
fields should help; tokenizer.tokenizer.tokenize_headers doesn't do so only
because my testing corpora are such that i can't add more headers without
getting major benefits for bogus reasons.

apart from all that, you said you're skipping received.  by several
accounts, that may be the most valuable of all the header fields.  i'm
(meaning tokenizer.tokenizer.tokenize_headers) skipping them too for the
reason explained above.  offline a week or two ago, neil schemenauer
reported good results from this scheme:

    ip_re = re.compile(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})')

    for header in msg.get_all("received", ()):
        for ip in ip_re.findall(header):
            parts = ip.split(".")
            for n in range(1, 5):
                yield 'received:' + '.'.join(parts[:n])

this makes a lot of sense to me; i just checked it in, but left it disabled
for now.

