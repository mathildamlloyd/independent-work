None[Spambayes] all but one testingTim Peters wrote:
> I've run no experiments on training set size yet, and won't hazard a guess
> as to how much is enough.  I'm nearly certain that the 4000h+2750s I've been
> using is way more than enough, though.

Okay, I believe you.

> Each call to learn() and to unlearn() computes a new probability for every
> word in the database.  There's an official way to avoid that in the first
> two loops, e.g.
> 
>     for msg in spam:
>         gb.learn(msg, True, False)
>     gb.update_probabilities()

I did that.  It's still really slow when you have thousands of messages.

> In each of the last two loops, the total # of ham and total # of spam in the
> "learned" set is invariant across loop trips, and you *could* break into the
> abstraction to exploit that:  the only probabilities that actually change
> across those loop trips are those associated with the words in msg.  Then
> the runtime for each trip would be proportional to the # of words in the msg
> rather than the number of words in the database.

I hadn't tried that.  I figured it was better to find out if "all but
one" testing had any appreciable value.  It looks like it doesn't so
I'll forget about it.

> Another area for potentially fruitful study:  it's clear that the
> highest-value indicators usually appear "early" in msgs, and for spam
> there's an actual reason for that:  advertising has to strive to get your
> attention early.  So, for example, if we only bothered to tokenize the first
> 90% of a msg, would results get worse?

Spammers could exploit this including a large MIME part at the beginning
of the message.  In pratice that would probably work fine.  

> sometimes an on-topic message starts well but then rambles.

Never.  I remember the time when I was ten years old and went down to
the fishing hole with my buddies.  This guy named Gordon had a really
huge head.  Wait, maybe that was Joe.  Well, no matter.  As I recall, it
was a hot day and everyone was tired...Human Growth Hormone...girl with
huge breasts...blah blah blah......

[spambayes] all but one testingtim peters wrote:
> i've run no experiments on training set size yet, and won't hazard a guess
> as to how much is enough.  i'm nearly certain that the 4000h+2750s i've been
> using is way more than enough, though.

okay, i believe you.

> each call to learn() and to unlearn() computes a new probability for every
> word in the database.  there's an official way to avoid that in the first
> two loops, e.g.
> 
>     for msg in spam:
>         gb.learn(msg, true, false)
>     gb.update_probabilities()

i did that.  it's still really slow when you have thousands of messages.

> in each of the last two loops, the total # of ham and total # of spam in the
> "learned" set is invariant across loop trips, and you *could* break into the
> abstraction to exploit that:  the only probabilities that actually change
> across those loop trips are those associated with the words in msg.  then
> the runtime for each trip would be proportional to the # of words in the msg
> rather than the number of words in the database.

i hadn't tried that.  i figured it was better to find out if "all but
one" testing had any appreciable value.  it looks like it doesn't so
i'll forget about it.

> another area for potentially fruitful study:  it's clear that the
> highest-value indicators usually appear "early" in msgs, and for spam
> there's an actual reason for that:  advertising has to strive to get your
> attention early.  so, for example, if we only bothered to tokenize the first
> 90% of a msg, would results get worse?

spammers could exploit this including a large mime part at the beginning
of the message.  in pratice that would probably work fine.  

> sometimes an on-topic message starts well but then rambles.

never.  i remember the time when i was ten years old and went down to
the fishing hole with my buddies.  this guy named gordon had a really
huge head.  wait, maybe that was joe.  well, no matter.  as i recall, it
was a hot day and everyone was tired...human growth hormone...girl with
huge breasts...blah blah blah......

